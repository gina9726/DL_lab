{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network classification in MNIST digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf                                                         \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Define training parameter\n",
    "max_iter = 10000\n",
    "batch_size = 100\n",
    "\n",
    "# get MNIST data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    # YOUR CODE HERE >>>>>>\n",
    "    Weights = ???\n",
    "    biases = ???\n",
    "    Wx_plus_b = ???\n",
    "    # <<<<<<\n",
    "    # apply dropout and the given activation function\n",
    "    if activation_function is None:\n",
    "        outputs = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
    "    else:\n",
    "        outputs = tf.nn.dropout(activation_function(Wx_plus_b), keep_prob)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(x, y):\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: x, keep_prob: 1.})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: x, ys: y, keep_prob: 1.})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Iteration: 0, Accuracy: 0.14390000701\n",
      "Iteration: 100, Accuracy: 0.658599972725\n",
      "Iteration: 200, Accuracy: 0.776199996471\n",
      "Iteration: 300, Accuracy: 0.813499987125\n",
      "Iteration: 400, Accuracy: 0.841099977493\n",
      "Iteration: 500, Accuracy: 0.855700016022\n",
      "Iteration: 600, Accuracy: 0.862800002098\n",
      "Iteration: 700, Accuracy: 0.868600010872\n",
      "Iteration: 800, Accuracy: 0.869099974632\n",
      "Iteration: 900, Accuracy: 0.873199999332\n",
      "Iteration: 1000, Accuracy: 0.877099990845\n",
      "Iteration: 1100, Accuracy: 0.87760001421\n",
      "Iteration: 1200, Accuracy: 0.885900020599\n",
      "Iteration: 1300, Accuracy: 0.881900012493\n",
      "Iteration: 1400, Accuracy: 0.887300014496\n",
      "Iteration: 1500, Accuracy: 0.888599991798\n",
      "Iteration: 1600, Accuracy: 0.884199976921\n",
      "Iteration: 1700, Accuracy: 0.88639998436\n",
      "Iteration: 1800, Accuracy: 0.888899981976\n",
      "Iteration: 1900, Accuracy: 0.89209997654\n",
      "Iteration: 2000, Accuracy: 0.893700003624\n",
      "Iteration: 2100, Accuracy: 0.888800024986\n",
      "Iteration: 2200, Accuracy: 0.88789999485\n",
      "Iteration: 2300, Accuracy: 0.894200026989\n",
      "Iteration: 2400, Accuracy: 0.887700021267\n",
      "Iteration: 2500, Accuracy: 0.887099981308\n",
      "Iteration: 2600, Accuracy: 0.896300017834\n",
      "Iteration: 2700, Accuracy: 0.895500004292\n",
      "Iteration: 2800, Accuracy: 0.898299992085\n",
      "Iteration: 2900, Accuracy: 0.893400013447\n",
      "Iteration: 3000, Accuracy: 0.903500020504\n",
      "Iteration: 3100, Accuracy: 0.896600008011\n",
      "Iteration: 3200, Accuracy: 0.893700003624\n",
      "Iteration: 3300, Accuracy: 0.896399974823\n",
      "Iteration: 3400, Accuracy: 0.892300009727\n",
      "Iteration: 3500, Accuracy: 0.896099984646\n",
      "Iteration: 3600, Accuracy: 0.899999976158\n",
      "Iteration: 3700, Accuracy: 0.89620000124\n",
      "Iteration: 3800, Accuracy: 0.897800028324\n",
      "Iteration: 3900, Accuracy: 0.895399987698\n",
      "Iteration: 4000, Accuracy: 0.903999984264\n",
      "Iteration: 4100, Accuracy: 0.893299996853\n",
      "Iteration: 4200, Accuracy: 0.897000014782\n",
      "Iteration: 4300, Accuracy: 0.900099992752\n",
      "Iteration: 4400, Accuracy: 0.901499986649\n",
      "Iteration: 4500, Accuracy: 0.900699973106\n",
      "Iteration: 4600, Accuracy: 0.904500007629\n",
      "Iteration: 4700, Accuracy: 0.894100010395\n",
      "Iteration: 4800, Accuracy: 0.901099979877\n",
      "Iteration: 4900, Accuracy: 0.899900019169\n",
      "Iteration: 5000, Accuracy: 0.893000006676\n",
      "Iteration: 5100, Accuracy: 0.899999976158\n",
      "Iteration: 5200, Accuracy: 0.899999976158\n",
      "Iteration: 5300, Accuracy: 0.898999989033\n",
      "Iteration: 5400, Accuracy: 0.904299974442\n",
      "Iteration: 5500, Accuracy: 0.905799984932\n",
      "Iteration: 5600, Accuracy: 0.900499999523\n",
      "Iteration: 5700, Accuracy: 0.898699998856\n",
      "Iteration: 5800, Accuracy: 0.908399999142\n",
      "Iteration: 5900, Accuracy: 0.904500007629\n",
      "Iteration: 6000, Accuracy: 0.897800028324\n",
      "Iteration: 6100, Accuracy: 0.897099971771\n",
      "Iteration: 6200, Accuracy: 0.90909999609\n",
      "Iteration: 6300, Accuracy: 0.895500004292\n",
      "Iteration: 6400, Accuracy: 0.908299982548\n",
      "Iteration: 6500, Accuracy: 0.896799981594\n",
      "Iteration: 6600, Accuracy: 0.907400012016\n",
      "Iteration: 6700, Accuracy: 0.904799997807\n",
      "Iteration: 6800, Accuracy: 0.908500015736\n",
      "Iteration: 6900, Accuracy: 0.907800018787\n",
      "Iteration: 7000, Accuracy: 0.901600003242\n",
      "Iteration: 7100, Accuracy: 0.90030002594\n",
      "Iteration: 7200, Accuracy: 0.904399991035\n",
      "Iteration: 7300, Accuracy: 0.893499970436\n",
      "Iteration: 7400, Accuracy: 0.901700019836\n",
      "Iteration: 7500, Accuracy: 0.901600003242\n",
      "Iteration: 7600, Accuracy: 0.89099997282\n",
      "Iteration: 7700, Accuracy: 0.902700006962\n",
      "Iteration: 7800, Accuracy: 0.908100008965\n",
      "Iteration: 7900, Accuracy: 0.9049000144\n",
      "Iteration: 8000, Accuracy: 0.901300013065\n",
      "Iteration: 8100, Accuracy: 0.906199991703\n",
      "Iteration: 8200, Accuracy: 0.902999997139\n",
      "Iteration: 8300, Accuracy: 0.906799972057\n",
      "Iteration: 8400, Accuracy: 0.907299995422\n",
      "Iteration: 8500, Accuracy: 0.906599998474\n",
      "Iteration: 8600, Accuracy: 0.906099975109\n",
      "Iteration: 8700, Accuracy: 0.904200017452\n",
      "Iteration: 8800, Accuracy: 0.905399978161\n",
      "Iteration: 8900, Accuracy: 0.906700015068\n",
      "Iteration: 9000, Accuracy: 0.909399986267\n",
      "Iteration: 9100, Accuracy: 0.907800018787\n",
      "Iteration: 9200, Accuracy: 0.904500007629\n",
      "Iteration: 9300, Accuracy: 0.89929997921\n",
      "Iteration: 9400, Accuracy: 0.907999992371\n",
      "Iteration: 9500, Accuracy: 0.904799997807\n",
      "Iteration: 9600, Accuracy: 0.902199983597\n",
      "Iteration: 9700, Accuracy: 0.900600016117\n",
      "Iteration: 9800, Accuracy: 0.90649998188\n",
      "Iteration: 9900, Accuracy: 0.903900027275\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, 784])    # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "prediction = add_layer(xs, 784, 10)\n",
    "\n",
    "# the error between prediction and real data\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=prediction)\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(max_iter):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})\n",
    "    if i%100 == 0:\n",
    "        accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "        print('Iteration: {}, Accuracy: {}'.format(i, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
