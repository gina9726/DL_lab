{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network classification in MNIST digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf                                                         \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Define training parameter\n",
    "max_iter = 10000\n",
    "batch_size = 100\n",
    "\n",
    "# get MNIST data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.add(tf.matmul(inputs, Weights),biases)\n",
    "    # apply dropout and the given activation function\n",
    "    if activation_function is None:\n",
    "        outputs = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
    "    else:\n",
    "        outputs = tf.nn.dropout(activation_function(Wx_plus_b), keep_prob)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(x, y):\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: x, keep_prob: 1.})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: x, ys: y, keep_prob: 1.})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Iteration: 0, Accuracy: 0.113700002432\n",
      "Iteration: 100, Accuracy: 0.148399993777\n",
      "Iteration: 200, Accuracy: 0.238700002432\n",
      "Iteration: 300, Accuracy: 0.338499993086\n",
      "Iteration: 400, Accuracy: 0.421700000763\n",
      "Iteration: 500, Accuracy: 0.494800001383\n",
      "Iteration: 600, Accuracy: 0.541999995708\n",
      "Iteration: 700, Accuracy: 0.586199998856\n",
      "Iteration: 800, Accuracy: 0.622099995613\n",
      "Iteration: 900, Accuracy: 0.646300017834\n",
      "Iteration: 1000, Accuracy: 0.666999995708\n",
      "Iteration: 1100, Accuracy: 0.686800003052\n",
      "Iteration: 1200, Accuracy: 0.702099978924\n",
      "Iteration: 1300, Accuracy: 0.718699991703\n",
      "Iteration: 1400, Accuracy: 0.73140001297\n",
      "Iteration: 1500, Accuracy: 0.745500028133\n",
      "Iteration: 1600, Accuracy: 0.755500018597\n",
      "Iteration: 1700, Accuracy: 0.763499975204\n",
      "Iteration: 1800, Accuracy: 0.771899998188\n",
      "Iteration: 1900, Accuracy: 0.777000010014\n",
      "Iteration: 2000, Accuracy: 0.783900022507\n",
      "Iteration: 2100, Accuracy: 0.789399981499\n",
      "Iteration: 2200, Accuracy: 0.795799970627\n",
      "Iteration: 2300, Accuracy: 0.800400018692\n",
      "Iteration: 2400, Accuracy: 0.805400013924\n",
      "Iteration: 2500, Accuracy: 0.810599982738\n",
      "Iteration: 2600, Accuracy: 0.815400004387\n",
      "Iteration: 2700, Accuracy: 0.817799985409\n",
      "Iteration: 2800, Accuracy: 0.820999979973\n",
      "Iteration: 2900, Accuracy: 0.827300012112\n",
      "Iteration: 3000, Accuracy: 0.828800022602\n",
      "Iteration: 3100, Accuracy: 0.831700026989\n",
      "Iteration: 3200, Accuracy: 0.836000025272\n",
      "Iteration: 3300, Accuracy: 0.839800000191\n",
      "Iteration: 3400, Accuracy: 0.839299976826\n",
      "Iteration: 3500, Accuracy: 0.843200027943\n",
      "Iteration: 3600, Accuracy: 0.845799982548\n",
      "Iteration: 3700, Accuracy: 0.848399996758\n",
      "Iteration: 3800, Accuracy: 0.8492000103\n",
      "Iteration: 3900, Accuracy: 0.850499987602\n",
      "Iteration: 4000, Accuracy: 0.853699982166\n",
      "Iteration: 4100, Accuracy: 0.855300009251\n",
      "Iteration: 4200, Accuracy: 0.856000006199\n",
      "Iteration: 4300, Accuracy: 0.852999985218\n",
      "Iteration: 4400, Accuracy: 0.856800019741\n",
      "Iteration: 4500, Accuracy: 0.858500003815\n",
      "Iteration: 4600, Accuracy: 0.860599994659\n",
      "Iteration: 4700, Accuracy: 0.860199987888\n",
      "Iteration: 4800, Accuracy: 0.863900005817\n",
      "Iteration: 4900, Accuracy: 0.864600002766\n",
      "Iteration: 5000, Accuracy: 0.865000009537\n",
      "Iteration: 5100, Accuracy: 0.864400029182\n",
      "Iteration: 5200, Accuracy: 0.866900026798\n",
      "Iteration: 5300, Accuracy: 0.868700027466\n",
      "Iteration: 5400, Accuracy: 0.869400024414\n",
      "Iteration: 5500, Accuracy: 0.871299982071\n",
      "Iteration: 5600, Accuracy: 0.870299994946\n",
      "Iteration: 5700, Accuracy: 0.872099995613\n",
      "Iteration: 5800, Accuracy: 0.872300028801\n",
      "Iteration: 5900, Accuracy: 0.873099982738\n",
      "Iteration: 6000, Accuracy: 0.873099982738\n",
      "Iteration: 6100, Accuracy: 0.873199999332\n",
      "Iteration: 6200, Accuracy: 0.874100029469\n",
      "Iteration: 6300, Accuracy: 0.871800005436\n",
      "Iteration: 6400, Accuracy: 0.875299990177\n",
      "Iteration: 6500, Accuracy: 0.876600027084\n",
      "Iteration: 6600, Accuracy: 0.875800013542\n",
      "Iteration: 6700, Accuracy: 0.876800000668\n",
      "Iteration: 6800, Accuracy: 0.875100016594\n",
      "Iteration: 6900, Accuracy: 0.877399981022\n",
      "Iteration: 7000, Accuracy: 0.876299977303\n",
      "Iteration: 7100, Accuracy: 0.876800000668\n",
      "Iteration: 7200, Accuracy: 0.876399993896\n",
      "Iteration: 7300, Accuracy: 0.879400014877\n",
      "Iteration: 7400, Accuracy: 0.879999995232\n",
      "Iteration: 7500, Accuracy: 0.879299998283\n",
      "Iteration: 7600, Accuracy: 0.880200028419\n",
      "Iteration: 7700, Accuracy: 0.878199994564\n",
      "Iteration: 7800, Accuracy: 0.878799974918\n",
      "Iteration: 7900, Accuracy: 0.8791000247\n",
      "Iteration: 8000, Accuracy: 0.880299985409\n",
      "Iteration: 8100, Accuracy: 0.879999995232\n",
      "Iteration: 8200, Accuracy: 0.882099986076\n",
      "Iteration: 8300, Accuracy: 0.880200028419\n",
      "Iteration: 8400, Accuracy: 0.882700026035\n",
      "Iteration: 8500, Accuracy: 0.881600022316\n",
      "Iteration: 8600, Accuracy: 0.883400022984\n",
      "Iteration: 8700, Accuracy: 0.882099986076\n",
      "Iteration: 8800, Accuracy: 0.88330000639\n",
      "Iteration: 8900, Accuracy: 0.882000029087\n",
      "Iteration: 9000, Accuracy: 0.88220000267\n",
      "Iteration: 9100, Accuracy: 0.883899986744\n",
      "Iteration: 9200, Accuracy: 0.88330000639\n",
      "Iteration: 9300, Accuracy: 0.882799983025\n",
      "Iteration: 9400, Accuracy: 0.881099998951\n",
      "Iteration: 9500, Accuracy: 0.884199976921\n",
      "Iteration: 9600, Accuracy: 0.884100019932\n",
      "Iteration: 9700, Accuracy: 0.884000003338\n",
      "Iteration: 9800, Accuracy: 0.885500013828\n",
      "Iteration: 9900, Accuracy: 0.884599983692\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, 784])    # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "prediction = add_layer(xs, 784, 10)\n",
    "\n",
    "# the error between prediction and real data\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=prediction)\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(max_iter):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})\n",
    "    if i%100 == 0:\n",
    "        accuracy = compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "        print('Iteration: {}, Accuracy: {}'.format(i, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
