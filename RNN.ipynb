{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import loader #loader for mnist dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.ops import rnn\n",
    "import numpy as np\n",
    "import pdb, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist dataset\n",
    "\n",
    "The MNIST database of handwritten digits. [[website]](http://yann.lecun.com/exdb/mnist/)<br>\n",
    "There are **60,000** training images and **10,000** testing images in this dataset.<br>\n",
    "Each digit is a one-channel image. Size of image = 28*28 = 784.\n",
    "\n",
    "![](imgs/mnist_ex.png)\n",
    "\n",
    "There are some build-in mnist function can be used in tensorflow.\n",
    "\n",
    "Ex.<br>\n",
    "from tensorflow.examples.tutorials.mnist import input_data<br>\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "Instead of using these functions, I'll use the orginal dataset manually in this code.<br>\n",
    "It's more clear to trace the data-processing.\n",
    "\n",
    "When we load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load mnist data manually\n",
    "# loading 'train' or 'test' data\n",
    "# ex. load_mnist_data('train')\n",
    "# return images, labels and mean of all images. (But, we'll only use the mean of training data.)\n",
    "# ims: [N * 784]\n",
    "# labels: [N]\n",
    "# ims_mean: [784]\n",
    "\n",
    "def load_mnist_data(flag, data_path='data'):\n",
    "    data_loader = loader.MNIST(data_path)\n",
    "    if flag == 'train':\n",
    "        ims, labels = data_loader.load_training()\n",
    "    elif flag == 'test':\n",
    "        ims, labels = data_loader.load_testing()\n",
    "    else:\n",
    "        raise ValueError(\"Error. Only training or testing data.\")\n",
    "    ims = ims/255.0\n",
    "    ims_mean = np.mean(ims, axis=0)\n",
    "    ims = np.reshape(ims, [len(ims),28,28])\n",
    "    ims_mean = np.reshape(ims_mean,(28,28))\n",
    "    return ims, labels, ims_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1\n",
    "batch_size = 100   # training batch size\n",
    "test_batch_size = 100\n",
    "display_step = 50  # testing \n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "stddev=0.01    # standard deviation for random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions of Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    rnn_cell = tf.nn.rnn_cell\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_steps, n_input])  # mnist input images, [batch_size x 28 x 28]\n",
    "    y = tf.placeholder(tf.int32,[None])              # label, [batch_size]\n",
    "    # Declare Variables \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    ###### Define a LSTM cell with tensorflow #######\n",
    "    lstm_cell = ???\n",
    "    # Initial zero state\n",
    "    state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    for i in range(n_steps):\n",
    "        # Get RNN cell output\n",
    "        with tf.variable_scope('lstm', reuse=True if i > 0 else None):\n",
    "            output, state = lstm_cell(x[:,i,:], state)\n",
    "    \n",
    "    pred = tf.matmul(output, weights['out']) + biases['out']\n",
    "\n",
    "    probs = tf.nn.softmax(pred)\n",
    "    \n",
    "    one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=pred)\n",
    "        \n",
    "    loss = tf.reduce_mean(cross_entropy_loss)\n",
    "    \n",
    "    return x, y, loss, pred, one_hot_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(sess, x, y, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter):\n",
    "    Train_Loss = 0\n",
    "    Test_Loss = 0\n",
    "    Train_Acc = 0\n",
    "    Test_Acc = 0\n",
    "    for idx in xrange(iter_per_epoch):\n",
    "        batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "        batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        Train_Loss += C/batch_size   # calculate the loss in average (per image).\n",
    "        Train_Acc += A\n",
    "    # Eval testing dataset\n",
    "    for idx in xrange(test_iter):\n",
    "        batch_xs = ims_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]] - ims_mean\n",
    "        batch_ys = labels_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        Test_Loss += C/test_batch_size\n",
    "        Test_Acc += A\n",
    "    return Train_Loss, Train_Acc, Test_Loss, Test_Acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------After Random Initialization------\n",
      "Training: loss=0.027734, acc=0.133867.\t\tTesting: loss=0.027780, acc=0.128100\n",
      " 3.053518 seconds\n",
      "------Start Training------\n",
      "Epoch 0.000000, Training: loss=0.023228, acc=0.256633.\t\tTesting: loss=0.023174, acc=0.253600\n",
      "Epoch 0.083333, Training: loss=0.005681, acc=0.819167.\t\tTesting: loss=0.005497, acc=0.821400\n",
      "Epoch 0.166667, Training: loss=0.004523, acc=0.856267.\t\tTesting: loss=0.004501, acc=0.860200\n",
      "Epoch 0.250000, Training: loss=0.002892, acc=0.910117.\t\tTesting: loss=0.002611, acc=0.914800\n",
      "Epoch 0.333333, Training: loss=0.002684, acc=0.921467.\t\tTesting: loss=0.002607, acc=0.926100\n",
      "Epoch 0.416667, Training: loss=0.001506, acc=0.954333.\t\tTesting: loss=0.001574, acc=0.951500\n",
      "Epoch 0.500000, Training: loss=0.001519, acc=0.954133.\t\tTesting: loss=0.001548, acc=0.952500\n",
      "Epoch 0.583333, Training: loss=0.001164, acc=0.965200.\t\tTesting: loss=0.001181, acc=0.964600\n",
      "Epoch 0.666667, Training: loss=0.001239, acc=0.963867.\t\tTesting: loss=0.001390, acc=0.960500\n",
      "Epoch 0.750000, Training: loss=0.001095, acc=0.967400.\t\tTesting: loss=0.001099, acc=0.967500\n",
      "Epoch 0.833333, Training: loss=0.001162, acc=0.964783.\t\tTesting: loss=0.001107, acc=0.965300\n",
      "Epoch 0.916667, Training: loss=0.000966, acc=0.971950.\t\tTesting: loss=0.000951, acc=0.973400\n",
      "Epoch 1, Training: loss=0.000945, acc=0.972133.\t\tTesting: loss=0.000878, acc=0.973500\n",
      "Cost 41.605405 seconds\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# loading training and testing data\n",
    "ims, labels, ims_mean = load_mnist_data('train', data_path='MNIST_data')\n",
    "#ims_mean = np.zeros((28*28))\n",
    "ims_test, labels_test, _ = load_mnist_data('test', data_path='MNIST_data')\n",
    "\n",
    "order_list = range(len(ims))\n",
    "\n",
    "# parameters related to mnist dataset \n",
    "test_iter = len(ims_test)/test_batch_size # number of testing-minibatch.\n",
    "\n",
    "iter_per_epoch = len(ims)/batch_size      # number of training-minibatch.\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    x, y, cost, pred, one_hot_y = RNN()\n",
    "    train_loss = cost/batch_size # loss per image\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(one_hot_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "    # initialize all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    Loss_plt = {'x':[], 'train_y':[], 'test_y':[]}\n",
    "    Acc_plt  = {'x':[], 'train_y':[], 'test_y':[]} \n",
    "    # Before Training (Random initialization), Evaluate the model one-time.\n",
    "    begin = time.time()\n",
    "    Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "    print \"------After Random Initialization------\"\n",
    "    print \"Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\" %(Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch,\n",
    "                                                                     Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "    \n",
    "    epoch = 0\n",
    "    step = 0      \n",
    "    \n",
    "    Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "    Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "    Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "    Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "    \n",
    "    duration = time.time()-begin\n",
    "    print \" %f seconds\"%(duration)\n",
    "    \n",
    "    print \"------Start Training------\"\n",
    "\n",
    "    for epoch in xrange(training_epochs):\n",
    "        begin = time.time()\n",
    "        Train_Loss = 0\n",
    "        Test_Loss = 0\n",
    "        Train_Acc = 0\n",
    "        Test_Acc = 0\n",
    "        for idx in xrange(iter_per_epoch):\n",
    "            batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "            batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run([optimizer], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            if step % display_step == 0:\n",
    "                Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "                print \"Epoch %f, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(float(step)/iter_per_epoch, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "                Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "                Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "                Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "                Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "            step += 1\n",
    "        \n",
    "        # Evaluate after each epoch finished.\n",
    "        Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "        print \"Epoch %d, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(epoch+1, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "        Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "        Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "        Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "        Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "        duration = time.time()-begin\n",
    "        print \"Cost %f seconds\"%(duration)\n",
    "    print(\"Optimization Finished!\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Possible Results\n",
    "------After Random Initialization------\n",
    "Training: loss=2.853943, acc=0.085467.\t\tTesting: loss=2.856901, acc=0.090800\n",
    " 8.238626 seconds\n",
    "------Start Training------\n",
    "Epoch 0.000000, Training: loss=2.386962, acc=0.267517.\t\tTesting: loss=2.363946, acc=0.267900\n",
    "Epoch 0.083333, Training: loss=0.475765, acc=0.853267.\t\tTesting: loss=0.464164, acc=0.860400\n",
    "Epoch 0.166667, Training: loss=0.344172, acc=0.894317.\t\tTesting: loss=0.349722, acc=0.893500\n",
    "Epoch 0.250000, Training: loss=0.251788, acc=0.922267.\t\tTesting: loss=0.239763, acc=0.924100\n",
    "Epoch 0.333333, Training: loss=0.185987, acc=0.942233.\t\tTesting: loss=0.187578, acc=0.941400\n",
    "Epoch 0.416667, Training: loss=0.142461, acc=0.955950.\t\tTesting: loss=0.142172, acc=0.956500\n",
    "Epoch 0.500000, Training: loss=0.135455, acc=0.958617.\t\tTesting: loss=0.137519, acc=0.956900\n",
    "Epoch 0.583333, Training: loss=0.118036, acc=0.964783.\t\tTesting: loss=0.125609, acc=0.960200\n",
    "Epoch 0.666667, Training: loss=0.135859, acc=0.961700.\t\tTesting: loss=0.146974, acc=0.956000\n",
    "Epoch 0.750000, Training: loss=0.112599, acc=0.966583.\t\tTesting: loss=0.119863, acc=0.963200\n",
    "Epoch 0.833333, Training: loss=0.127726, acc=0.960500.\t\tTesting: loss=0.133251, acc=0.959300\n",
    "Epoch 0.916667, Training: loss=0.098880, acc=0.969883.\t\tTesting: loss=0.105006, acc=0.968000\n",
    "Epoch 1, Training: loss=0.099234, acc=0.969933.\t\tTesting: loss=0.111800, acc=0.966600\n",
    "Cost 126.912962 seconds\n",
    "Optimization Finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
