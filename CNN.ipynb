{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#A Convolutional Network implementation example using TensorFlow library.\n",
    "#This example is using the MNIST database of handwritten digits\n",
    "#(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "#Author: Aymeric Damien\n",
    "#Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "# Based on above project, modified by James Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from mnist import loader #loader for mnist dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pdb, time, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist dataset\n",
    "\n",
    "The MNIST database of handwritten digits. [[website]](http://yann.lecun.com/exdb/mnist/)<br>\n",
    "There are **60,000** training images and **10,000** testing images in this dataset.<br>\n",
    "Each digit is a one-channel image. Size of image = 28*28 = 784.\n",
    "\n",
    "![](imgs/mnist_ex.png)\n",
    "\n",
    "There are some build-in mnist function can be used in tensorflow.\n",
    "\n",
    "Ex.<br>\n",
    "from tensorflow.examples.tutorials.mnist import input_data<br>\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "Instead of using these functions, I'll use the orginal dataset manually in this code.<br>\n",
    "It's more clear to trace the data-processing.\n",
    "\n",
    "When we load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load mnist data manually\n",
    "# loading 'train' or 'test' data\n",
    "# ex. load_mnist_data('train')\n",
    "# return images, labels and mean of all images. (But, we'll only use the mean of training data.)\n",
    "# ims: [N * 784]\n",
    "# labels: [N]\n",
    "# ims_mean: [784]\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def load_mnist_data(flag, data_path='data'):\n",
    "    data_loader = loader.MNIST(data_path)\n",
    "    if flag == 'train':\n",
    "        ims, labels = data_loader.load_training()\n",
    "    elif flag == 'test':\n",
    "        ims, labels = data_loader.load_testing()\n",
    "    else:\n",
    "        raise ValueError(\"Error. Only training or testing data.\")\n",
    "    ims = ims/255.0\n",
    "    ims_mean = np.mean(ims, axis=0)\n",
    "    return ims, labels, ims_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001  # default = 0.001\n",
    "training_epochs = 1 # default = 1\n",
    "batch_size = 64   # training batch size, default = 64\n",
    "test_batch_size = 100\n",
    "display_step = 50  # testing, default = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "stddev=0.01    # standard deviation for random initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions of Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D wrapper, with bias and given activation\n",
    "def convolution_layer(x, kernel_shape, bias_shape, activation_function=None, strides=1):\n",
    "    # kernel_shape: [kernel_height, kernel_width, input_channel, output_channel]\n",
    "    weight = tf.Variable(tf.truncated_normal(kernel_shape, mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.random_normal(bias_shape))\n",
    "    output = tf.add(tf.nn.conv2d(x, weight, strides=[1, strides, strides, 1], padding='SAME'), bias)\n",
    "    if activation_function == None:\n",
    "        return output\n",
    "    else:\n",
    "        return activation_function(output)\n",
    "\n",
    "# MaxPool2D wrapper\n",
    "def maxpooling_layer(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "# Fully-connected wrapper, with bias and given activation\n",
    "def fully_connected_layer(x, kernel_shape, bias_shape, activation_function=None):    \n",
    "    weight = tf.Variable(tf.truncated_normal(kernel_shape, mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.random_normal(bias_shape))\n",
    "    output = tf.add(tf.matmul(x, weight), bias)\n",
    "    if activation_function == None:\n",
    "        return output\n",
    "    else:\n",
    "        return activation_function(output)\n",
    "\n",
    "                 \n",
    "def lenet():\n",
    "    \n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])  # mnist input images, [batch_size x 784]\n",
    "    y = tf.placeholder(tf.int32,[None])              # label, [batch_size]\n",
    "    dropout = tf.placeholder(tf.float32)  #dropout ratio\n",
    "    \n",
    "    # Construct model\n",
    "    x_reshape = tf.reshape(x, shape=[-1, 28, 28, 1]) # Transfer shape. Prepare for convolution\n",
    "\n",
    "    ######## PART I #########\n",
    "    # Convolution Layer (5x5 conv, 1 input, 32 outputs)\n",
    "    conv1 = ???\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool1 = ???\n",
    "\n",
    "    # Convolution Layer (5x5 conv, 32 inputs, 64 outputs)\n",
    "    conv2 = ???\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool2 = ???\n",
    "\n",
    "    ######## PART II #########\n",
    "    # Fully connected layer (7*7*64 inputs, 100 outputs)\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(pool2, [-1, 7*7*64])\n",
    "    fc1 = ???\n",
    "\n",
    "    # Fully connected layer(Output layer, 1024 inputs, 10 outputs (class prediction))\n",
    "    # Map the fc1 to the number of class prediction                        \n",
    "    logits = ???\n",
    "    probs = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    one_hot_y = tf.cast(one_hot_y, tf.float32)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y))\n",
    "        \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(probs, 1), tf.argmax(one_hot_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "    return x, y, dropout, loss, probs, accuracy, conv1, conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate whole dataset\n",
    "\n",
    "def eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter):\n",
    "    Train_Loss = 0\n",
    "    Test_Loss = 0\n",
    "    Train_Acc = 0\n",
    "    Test_Acc = 0\n",
    "    # Eval training dataset\n",
    "    for idx in xrange(iter_per_epoch):\n",
    "        batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "        batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Train_Loss += C/batch_size   # calculate the loss in average (per image).\n",
    "        Train_Acc += A\n",
    "    # Eval testing dataset\n",
    "    for idx in xrange(test_iter):\n",
    "        batch_xs = ims_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]] - ims_mean\n",
    "        batch_ys = labels_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Test_Loss += C/test_batch_size\n",
    "        Test_Acc += A\n",
    "    return Train_Loss, Train_Acc, Test_Loss, Test_Acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------After Random Initialization------\n",
      "Training: loss=0.042481, acc=0.099253.\t\tTesting: loss=0.027142, acc=0.103200\n",
      " 2.440453 seconds\n",
      "------Start Training------\n",
      "Epoch 0.05, Training: loss=0.036073, acc=0.112377.\t\tTesting: loss=0.023081, acc=0.113500\n",
      "Epoch 0.11, Training: loss=0.035291, acc=0.140675.\t\tTesting: loss=0.022583, acc=0.148400\n",
      "Epoch 0.16, Training: loss=0.014542, acc=0.683398.\t\tTesting: loss=0.009204, acc=0.686500\n",
      "Epoch 0.21, Training: loss=0.007848, acc=0.842266.\t\tTesting: loss=0.004826, acc=0.851800\n",
      "Epoch 0.27, Training: loss=0.005334, acc=0.892326.\t\tTesting: loss=0.003160, acc=0.901400\n",
      "Epoch 0.32, Training: loss=0.003923, acc=0.925494.\t\tTesting: loss=0.002321, acc=0.928000\n",
      "Epoch 0.37, Training: loss=0.003527, acc=0.929763.\t\tTesting: loss=0.002045, acc=0.934000\n",
      "Epoch 0.43, Training: loss=0.003007, acc=0.940068.\t\tTesting: loss=0.001760, acc=0.942900\n",
      "Epoch 0.48, Training: loss=0.003110, acc=0.941152.\t\tTesting: loss=0.001801, acc=0.943700\n",
      "Epoch 0.53, Training: loss=0.002817, acc=0.943103.\t\tTesting: loss=0.001624, acc=0.947200\n",
      "Epoch 0.59, Training: loss=0.002315, acc=0.956327.\t\tTesting: loss=0.001368, acc=0.957900\n",
      "Epoch 0.64, Training: loss=0.002885, acc=0.940418.\t\tTesting: loss=0.001706, acc=0.943500\n",
      "Epoch 0.69, Training: loss=0.002042, acc=0.959779.\t\tTesting: loss=0.001232, acc=0.958700\n",
      "Epoch 0.75, Training: loss=0.002296, acc=0.954226.\t\tTesting: loss=0.001345, acc=0.955500\n",
      "Epoch 0.80, Training: loss=0.002098, acc=0.957911.\t\tTesting: loss=0.001248, acc=0.959200\n",
      "Epoch 0.85, Training: loss=0.002059, acc=0.957477.\t\tTesting: loss=0.001267, acc=0.959100\n",
      "Epoch 0.91, Training: loss=0.001907, acc=0.962797.\t\tTesting: loss=0.001109, acc=0.964100\n",
      "Epoch 0.96, Training: loss=0.001696, acc=0.965898.\t\tTesting: loss=0.001031, acc=0.967600\n",
      "Epoch 1, Training: loss=0.001858, acc=0.961696.\t\tTesting: loss=0.001101, acc=0.962800\n",
      "Cost 54.661427 seconds\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# loading training and testing data\n",
    "ims, labels, ims_mean = load_mnist_data('train', data_path='MNIST_data')\n",
    "#ims_mean = np.zeros((28*28))\n",
    "ims_test, labels_test, _ = load_mnist_data('test', data_path='MNIST_data')\n",
    "\n",
    "order_list = range(len(ims))\n",
    "\n",
    "# parameters related to mnist dataset \n",
    "test_iter = len(ims_test)/test_batch_size # number of testing-minibatch.\n",
    "\n",
    "iter_per_epoch = len(ims)/batch_size      # number of training-minibatch.\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "#with tf.device(\"/gpu:0\"):\n",
    "#    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "#    sess = tf.Session(config = config)\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    x, y, dropout, cost, pred, accuracy, conv1, conv2= lenet()\n",
    "    train_loss = cost/batch_size # loss per image\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(train_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "    # initialize all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    Loss_plt = {'x':[], 'train_y':[], 'test_y':[]}\n",
    "    Acc_plt  = {'x':[], 'train_y':[], 'test_y':[]} \n",
    "    # Before Training (Random initialization), Evaluate the model one-time.\n",
    "    begin = time.time()\n",
    "    Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "    print \"------After Random Initialization------\"\n",
    "    print \"Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\" %(Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch,\n",
    "                                                                     Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "    \n",
    "    epoch = 0\n",
    "    step = 1      \n",
    "    \n",
    "    Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "    Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "    Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "    Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "    \n",
    "    duration = time.time()-begin\n",
    "    print \" %f seconds\"%(duration)\n",
    "    \n",
    "    print \"------Start Training------\"\n",
    "\n",
    "    for epoch in xrange(training_epochs):\n",
    "        begin = time.time()\n",
    "        Train_Loss = 0\n",
    "        Test_Loss = 0\n",
    "        Train_Acc = 0\n",
    "        Test_Acc = 0\n",
    "        for idx in xrange(iter_per_epoch):\n",
    "            batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "            batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "            # Run optimization op (backprop)\n",
    "            #print batch_ys.shape\n",
    "            sess.run([optimizer], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.5})\n",
    "            if step % display_step == 0:\n",
    "                Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "                print \"Epoch %.2f, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(float(step)/iter_per_epoch, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "                Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "                Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "                Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "                Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "            step += 1\n",
    "        \n",
    "        # Evaluate after each epoch finished.\n",
    "        Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "        print \"Epoch %d, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(epoch+1, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "        Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "        Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "        Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "        Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "\n",
    "        duration = time.time()-begin\n",
    "        print \"Cost %f seconds\"%(duration)\n",
    "        \n",
    "        # Save results to npz, change the filename by yourself.\n",
    "        np.savez('npzfiles/outfilename', loss=Loss_plt, acc=Acc_plt)\n",
    "        \n",
    "        # Visualiztion\n",
    "        Vis=False\n",
    "        # Vis\n",
    "        if Vis == True:\n",
    "            index = np.random.randint(len(ims))\n",
    "            batch_ys = np.zeros((1))\n",
    "            batch_ys[0] = labels[index]\n",
    "            C1, C2 = sess.run([conv1, conv2], feed_dict={x: ims[index][np.newaxis,:], y:batch_ys, dropout: 0.0})\n",
    "            print C1.shape, C2.shape\n",
    "            for i in xrange(32):\n",
    "                plt.subplot(10,10,i+1),plt.imshow(C1[0,:,:,i],cmap='Greys_r'),plt.title('CONV1')\n",
    "            for i in xrange(64):\n",
    "                plt.subplot(10,10,100-i),plt.imshow(C2[0,:,:,i],cmap='Greys_r'),plt.title('CONV2')\n",
    "            plt.show()                                 \n",
    "    print(\"Optimization Finished!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the figure to compare different results.\n",
    "# loading npzfile (saved by main function)\n",
    "# Usage : plot_fcn([npz1,npz2,npz3], tag='train'/'test')\n",
    "# Ex.\n",
    "# types=['batchsize5', 'batchsize50', 'batchsize500']\n",
    "# plot_fcn(types, tag='train')  \n",
    "def plot_fcn(types, tag='train'):\n",
    "    npzfiles=[]\n",
    "    colors = ['b-', 'r-', 'k-', 'g-', 'y-', 'c-', 'm-']\n",
    "    if len(types) > len(colors):\n",
    "        print \"only accept %d types\"%(len(color))\n",
    "        return \n",
    "    \n",
    "    for i in xrange(len(types)):\n",
    "        npz = np.load('npzfiles/'+types[i]+'.npz')\n",
    "        npzfiles.append(npz)\n",
    "    print npzfiles\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.title('Loss.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.axis([0, training_epochs, 0, 5.0])\n",
    "    for i in xrange(len(npzfiles)):\n",
    "        plt.plot(npzfiles[i]['loss'].item()['x'], npzfiles[i]['loss'].item()[tag+'_y'], colors[i], label=types[i])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('(%)')\n",
    "    plt.axis([0, training_epochs, 0, 1.0])\n",
    "    for i in xrange(len(npzfiles)):\n",
    "        plt.plot(npzfiles[i]['acc'].item()['x'], npzfiles[i]['acc'].item()[tag+'_y'], colors[i], label=types[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<numpy.lib.npyio.NpzFile object at 0x7f28699eb790>, <numpy.lib.npyio.NpzFile object at 0x7f28699eb650>, <numpy.lib.npyio.NpzFile object at 0x7f28699eb690>]\n"
     ]
    }
   ],
   "source": [
    "# See results\n",
    "types=['batchsize5', 'batchsize50', 'batchsize500']\n",
    "plot_fcn(types, tag='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Possible Results \n",
    "\n",
    "\n",
    "![](imgs/train_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensorflow python API\n",
    "\n",
    "### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "\n",
    "Given an **input tensor of shape [batch, in_height, in_width, in_channels]** and a **filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]**, this op performs the following:\n",
    "\n",
    "Flattens the filter to a 2-D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "\n",
    "Extracts image patches from the input tensor to form a virtual tensor of shape [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "\n",
    "In detail, with the default NHWC format,\n",
    "\n",
    "output[b, i, j, k] = sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] * filter[di, dj, q, k]\n",
    "\n",
    "\n",
    "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].\n",
    "\n",
    "**Args:**\n",
    "\n",
    "> input: A Tensor. Must be one of the following types: half, float32, float64.\n",
    "\n",
    "> filter: A Tensor. Must have the same type as input.\n",
    "\n",
    "> strides: A list of ints. 1-D of length 4. The stride of the sliding window for each dimension of input. Must be in the same order as the dimension specified with format.\n",
    "\n",
    "> padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "use_cudnn_on_gpu: An optional bool. Defaults to True.\n",
    "\n",
    "> data_format: An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\". Specify the data format of the input and output data. With the default format \"NHWC\", the data is stored in the order of: [batch, in_height, in_width,\n",
    "in_channels]. Alternatively, the format could be \"NCHW\", the data storage order of: [batch, in_channels, in_height, in_width].\n",
    "\n",
    "> name: A name for the operation (optional).\n",
    "\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "> A Tensor. Has the same type as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
