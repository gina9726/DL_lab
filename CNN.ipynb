{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#A Convolutional Network implementation example using TensorFlow library.\n",
    "#This example is using the MNIST database of handwritten digits\n",
    "#(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "#Author: Aymeric Damien\n",
    "#Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "# Based on above project, modified by James Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from mnist import loader #loader for mnist dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pdb, time, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist dataset\n",
    "\n",
    "The MNIST database of handwritten digits. [[website]](http://yann.lecun.com/exdb/mnist/)<br>\n",
    "There are **60,000** training images and **10,000** testing images in this dataset.<br>\n",
    "Each digit is a one-channel image. Size of image = 28*28 = 784.\n",
    "\n",
    "![](imgs/mnist_ex.png)\n",
    "\n",
    "There are some build-in mnist function can be used in tensorflow.\n",
    "\n",
    "Ex.<br>\n",
    "from tensorflow.examples.tutorials.mnist import input_data<br>\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "Instead of using these functions, I'll use the orginal dataset manually in this code.<br>\n",
    "It's more clear to trace the data-processing.\n",
    "\n",
    "When we load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load mnist data manually\n",
    "# loading 'train' or 'test' data\n",
    "# ex. load_mnist_data('train')\n",
    "# return images, labels and mean of all images. (But, we'll only use the mean of training data.)\n",
    "# ims: [N * 784]\n",
    "# labels: [N]\n",
    "# ims_mean: [784]\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def load_mnist_data(flag, data_path='data'):\n",
    "    data_loader = loader.MNIST(data_path)\n",
    "    if flag == 'train':\n",
    "        ims, labels = data_loader.load_training()\n",
    "    elif flag == 'test':\n",
    "        ims, labels = data_loader.load_testing()\n",
    "    else:\n",
    "        raise ValueError(\"Error. Only training or testing data.\")\n",
    "    ims = ims/255.0\n",
    "    ims_mean = np.mean(ims, axis=0)\n",
    "    return ims, labels, ims_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001  # default = 0.001\n",
    "training_epochs = 1 # default = 1\n",
    "batch_size = 50   # training batch size, default = 50\n",
    "test_batch_size = 100\n",
    "display_step = 50  # testing, default = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "stddev=0.01    # standard deviation for random initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions of Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "\n",
    "# convolutional function\n",
    "# input: \n",
    "# x=[batch_size, height, width, channels]\n",
    "# W(Weights)=tf.Variable(shape=[kernel_size, kernel_size, input_channel, output_channel])\n",
    "# b(Biases)=tf.Variable(shape=[output_channel])\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])  # mnist input images, [batch_size x 784]\n",
    "    y = tf.placeholder(tf.int32,[None])              # label, [batch_size]\n",
    "    dropout = tf.placeholder(tf.float32)  #dropout ratio\n",
    "    \n",
    "    # Declare Variables \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], mean=0, stddev=stddev)),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], mean=0, stddev=stddev)),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.truncated_normal([7*7*64, 1024], mean=0, stddev=stddev)),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.truncated_normal([1024, n_classes], mean=0, stddev=stddev))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),  # default stddev=1.0\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    x_reshape = tf.reshape(x, shape=[-1, 28, 28, 1]) # Transfer shape. Prepare for convolution\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x_reshape, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob = 1-dropout)   # dropout ratio --> keep ratio\n",
    "\n",
    "    # Output, class prediction\n",
    "    pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    #one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    #one_hot_y = tf.cast(one_hot_y, tf.float32)\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=one_hot_y))\n",
    "\n",
    "    \n",
    "\n",
    "    probs = tf.nn.softmax(pred)\n",
    "    log_probs = tf.log(probs + 1e-8)\n",
    "\n",
    "    one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    #print one_hot_y.get_shape()\n",
    "    #cross_entropy_loss = - tf.mul(y,log_probs)\n",
    "    cross_entropy_loss = - tf.multiply(tf.cast(one_hot_y, tf.float32),log_probs)\n",
    "    \n",
    "    loss = tf.reduce_sum(cross_entropy_loss)\n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(one_hot_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "    return x, y, dropout, loss, pred, accuracy, conv1, conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate whole dataset\n",
    "\n",
    "def eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter):\n",
    "    Train_Loss = 0\n",
    "    Test_Loss = 0\n",
    "    Train_Acc = 0\n",
    "    Test_Acc = 0\n",
    "    # Eval training dataset\n",
    "    for idx in xrange(iter_per_epoch):\n",
    "        batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "        batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Train_Loss += C/batch_size   # calculate the loss in average (per image).\n",
    "        Train_Acc += A\n",
    "    # Eval testing dataset\n",
    "    for idx in xrange(test_iter):\n",
    "        batch_xs = ims_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]] - ims_mean\n",
    "        batch_ys = labels_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Test_Loss += C/test_batch_size\n",
    "        Test_Acc += A\n",
    "    return Train_Loss, Train_Acc, Test_Loss, Test_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the figure to compare different results.\n",
    "# loading npzfile (saved by main function)\n",
    "# Usage : plot_fcn([npz1,npz2,npz3], tag='train'/'test')\n",
    "# Ex.\n",
    "# types=['batchsize5', 'batchsize50', 'batchsize500']\n",
    "# plot_fcn(types, tag='train')  \n",
    "def plot_fcn(types, tag='train'):\n",
    "    npzfiles=[]\n",
    "    colors = ['b-', 'r-', 'k-', 'g-', 'y-', 'c-', 'm-']\n",
    "    if len(types) > len(colors):\n",
    "        print \"only accept %d types\"%(len(color))\n",
    "        return \n",
    "    \n",
    "    for i in xrange(len(types)):\n",
    "        npz = np.load('npzfiles/'+types[i]+'.npz')\n",
    "        npzfiles.append(npz)\n",
    "    print npzfiles\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.title('Loss.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.axis([0, training_epochs, 0, 5.0])\n",
    "    for i in xrange(len(npzfiles)):\n",
    "        plt.plot(npzfiles[i]['loss'].item()['x'], npzfiles[i]['loss'].item()[tag+'_y'], colors[i], label=types[i])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('(%)')\n",
    "    plt.axis([0, training_epochs, 0, 1.0])\n",
    "    for i in xrange(len(npzfiles)):\n",
    "        plt.plot(npzfiles[i]['acc'].item()['x'], npzfiles[i]['acc'].item()[tag+'_y'], colors[i], label=types[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "------After Random Initialization------\n",
      "Training: loss=2.555751, acc=0.099150.\t\tTesting: loss=2.559240, acc=0.100900\n",
      " 4.718290 seconds\n",
      "------Start Training------\n",
      "Epoch 0.041667, Training: loss=2.324580, acc=0.097367.\t\tTesting: loss=2.325648, acc=0.098200\n",
      "Epoch 0.083333, Training: loss=1.930575, acc=0.479183.\t\tTesting: loss=1.922899, acc=0.488000\n",
      "Epoch 0.125000, Training: loss=0.681093, acc=0.804683.\t\tTesting: loss=0.673975, acc=0.813000\n",
      "Epoch 0.166667, Training: loss=0.499315, acc=0.842483.\t\tTesting: loss=0.478510, acc=0.854100\n",
      "Epoch 0.208333, Training: loss=0.396507, acc=0.875033.\t\tTesting: loss=0.373057, acc=0.882900\n",
      "Epoch 0.250000, Training: loss=0.338154, acc=0.896183.\t\tTesting: loss=0.314644, acc=0.906000\n",
      "Epoch 0.291667, Training: loss=0.263697, acc=0.915533.\t\tTesting: loss=0.246638, acc=0.922200\n",
      "Epoch 0.333333, Training: loss=0.238789, acc=0.923717.\t\tTesting: loss=0.220087, acc=0.929100\n",
      "Epoch 0.375000, Training: loss=0.223477, acc=0.930300.\t\tTesting: loss=0.212450, acc=0.935000\n",
      "Epoch 0.416667, Training: loss=0.225928, acc=0.929633.\t\tTesting: loss=0.215107, acc=0.932000\n",
      "Epoch 0.458333, Training: loss=0.199536, acc=0.936767.\t\tTesting: loss=0.185693, acc=0.940600\n",
      "Epoch 0.500000, Training: loss=0.173070, acc=0.945617.\t\tTesting: loss=0.160903, acc=0.948300\n",
      "Epoch 0.541667, Training: loss=0.166193, acc=0.947300.\t\tTesting: loss=0.157788, acc=0.949400\n",
      "Epoch 0.583333, Training: loss=0.170375, acc=0.948183.\t\tTesting: loss=0.165535, acc=0.948100\n",
      "Epoch 0.625000, Training: loss=0.163098, acc=0.949550.\t\tTesting: loss=0.151128, acc=0.952900\n",
      "Epoch 0.666667, Training: loss=0.166539, acc=0.946750.\t\tTesting: loss=0.158017, acc=0.949300\n",
      "Epoch 0.708333, Training: loss=0.161615, acc=0.948433.\t\tTesting: loss=0.155220, acc=0.948300\n",
      "Epoch 0.750000, Training: loss=0.137519, acc=0.956300.\t\tTesting: loss=0.130270, acc=0.956600\n",
      "Epoch 0.791667, Training: loss=0.134127, acc=0.956333.\t\tTesting: loss=0.130740, acc=0.957400\n",
      "Epoch 0.833333, Training: loss=0.126277, acc=0.959317.\t\tTesting: loss=0.113099, acc=0.962500\n",
      "Epoch 0.875000, Training: loss=0.122232, acc=0.961833.\t\tTesting: loss=0.117146, acc=0.962900\n",
      "Epoch 0.916667, Training: loss=0.127082, acc=0.961533.\t\tTesting: loss=0.119076, acc=0.961200\n",
      "Epoch 0.958333, Training: loss=0.115909, acc=0.963867.\t\tTesting: loss=0.112262, acc=0.965200\n",
      "Epoch 1.000000, Training: loss=0.128502, acc=0.958250.\t\tTesting: loss=0.120976, acc=0.961600\n",
      "Epoch 1, Training: loss=0.128502, acc=0.958250.\t\tTesting: loss=0.120976, acc=0.961600\n",
      "Cost 87.612440 seconds\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# loading training and testing data\n",
    "ims, labels, ims_mean = load_mnist_data('train', data_path='MNIST_data')\n",
    "#ims_mean = np.zeros((28*28))\n",
    "ims_test, labels_test, _ = load_mnist_data('test', data_path='MNIST_data')\n",
    "\n",
    "order_list = range(len(ims))\n",
    "\n",
    "# parameters related to mnist dataset \n",
    "test_iter = len(ims_test)/test_batch_size # number of testing-minibatch.\n",
    "\n",
    "iter_per_epoch = len(ims)/batch_size      # number of training-minibatch.\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "#with tf.device(\"/gpu:0\"):\n",
    "#    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "#    sess = tf.Session(config = config)\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    x, y, dropout, cost, pred, accuracy, conv1, conv2= lenet()\n",
    "    train_loss = cost/batch_size # loss per image\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(train_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "    # initialize all variables\n",
    "    try:\n",
    "        init = tf.initialize_all_variables()\n",
    "    except:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    Loss_plt = {'x':[], 'train_y':[], 'test_y':[]}\n",
    "    Acc_plt  = {'x':[], 'train_y':[], 'test_y':[]} \n",
    "    # Before Training (Random initialization), Evaluate the model one-time.\n",
    "    begin = time.time()\n",
    "    Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "    print \"------After Random Initialization------\"\n",
    "    print \"Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\" %(Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch,\n",
    "                                                                     Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "    \n",
    "    epoch = 0\n",
    "    step = 1      \n",
    "    \n",
    "    Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "    Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "    Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "    Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "    \n",
    "    duration = time.time()-begin\n",
    "    print \" %f seconds\"%(duration)\n",
    "    \n",
    "    print \"------Start Training------\"\n",
    "\n",
    "    for epoch in xrange(training_epochs):\n",
    "        begin = time.time()\n",
    "        Train_Loss = 0\n",
    "        Test_Loss = 0\n",
    "        Train_Acc = 0\n",
    "        Test_Acc = 0\n",
    "        for idx in xrange(iter_per_epoch):\n",
    "            batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "            batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "            # Run optimization op (backprop)\n",
    "            #print batch_ys.shape\n",
    "            sess.run([optimizer], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.5})\n",
    "            if step % display_step == 0:\n",
    "                Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "                print \"Epoch %f, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(float(step)/iter_per_epoch, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "                Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "                Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "                Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "                Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "            step += 1\n",
    "        \n",
    "        # Evaluate after each epoch finished.\n",
    "        Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "        print \"Epoch %d, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(epoch+1, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "        Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "        Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "        Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "        Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "\n",
    "        duration = time.time()-begin\n",
    "        print \"Cost %f seconds\"%(duration)\n",
    "        \n",
    "        # Save results to npz, change the filename by yourself.\n",
    "        np.savez('npzfiles/outfilename', loss=Loss_plt, acc=Acc_plt)\n",
    "        \n",
    "        # Visualiztion\n",
    "        Vis=False\n",
    "        # Vis\n",
    "        if Vis == True:\n",
    "            index = np.random.randint(len(ims))\n",
    "            batch_ys = np.zeros((1))\n",
    "            batch_ys[0] = labels[index]\n",
    "            C1, C2 = sess.run([conv1, conv2], feed_dict={x: ims[index][np.newaxis,:], y:batch_ys, dropout: 0.0})\n",
    "            print C1.shape, C2.shape\n",
    "            for i in xrange(32):\n",
    "                plt.subplot(10,10,i+1),plt.imshow(C1[0,:,:,i],cmap='Greys_r'),plt.title('CONV1')\n",
    "            for i in xrange(64):\n",
    "                plt.subplot(10,10,100-i),plt.imshow(C2[0,:,:,i],cmap='Greys_r'),plt.title('CONV2')\n",
    "            plt.show()                                 \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.title('Loss.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.axis([0, training_epochs, 0, 5.0])\n",
    "    plt.plot(Loss_plt['x'], Loss_plt['train_y'], 'b-', label='training')\n",
    "    plt.plot(Loss_plt['x'], Loss_plt['test_y'], 'r-', label='testing')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('(%)')\n",
    "    plt.axis([0, training_epochs, 0, 1.0])\n",
    "    plt.plot(Acc_plt['x'], Acc_plt['train_y'], 'b-', label='training')\n",
    "    plt.plot(Acc_plt['x'], Acc_plt['test_y'], 'r-', label='testing')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Visualize the output from 'CONV1' and 'CONV2'.\n",
    "\n",
    "![](imgs/visualize.png)\n",
    "\n",
    "![](imgs/visualize2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<numpy.lib.npyio.NpzFile object at 0x7f0b2c6dd3d0>, <numpy.lib.npyio.NpzFile object at 0x7f0b2c6dd590>, <numpy.lib.npyio.NpzFile object at 0x7f0b2c6dd9d0>]\n"
     ]
    }
   ],
   "source": [
    "# See results\n",
    "types=['batchsize5', 'batchsize50', 'batchsize500']\n",
    "plot_fcn(types, tag='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Possible Results \n",
    "\n",
    "\n",
    "![](imgs/train_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensorflow python API\n",
    "\n",
    "### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "\n",
    "Given an **input tensor of shape [batch, in_height, in_width, in_channels]** and a **filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]**, this op performs the following:\n",
    "\n",
    "Flattens the filter to a 2-D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "\n",
    "Extracts image patches from the input tensor to form a virtual tensor of shape [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "\n",
    "In detail, with the default NHWC format,\n",
    "\n",
    "output[b, i, j, k] = sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] * filter[di, dj, q, k]\n",
    "\n",
    "\n",
    "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].\n",
    "\n",
    "**Args:**\n",
    "\n",
    "> input: A Tensor. Must be one of the following types: half, float32, float64.\n",
    "\n",
    "> filter: A Tensor. Must have the same type as input.\n",
    "\n",
    "> strides: A list of ints. 1-D of length 4. The stride of the sliding window for each dimension of input. Must be in the same order as the dimension specified with format.\n",
    "\n",
    "> padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "use_cudnn_on_gpu: An optional bool. Defaults to True.\n",
    "\n",
    "> data_format: An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\". Specify the data format of the input and output data. With the default format \"NHWC\", the data is stored in the order of: [batch, in_height, in_width,\n",
    "in_channels]. Alternatively, the format could be \"NCHW\", the data storage order of: [batch, in_channels, in_height, in_width].\n",
    "\n",
    "> name: A name for the operation (optional).\n",
    "\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "> A Tensor. Has the same type as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
